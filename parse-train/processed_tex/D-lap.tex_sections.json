{
    "sections": {
        "doco:title": "Finite element approximation  and  iterative method solution of\nelliptic control problem with constraints to gradient of\nstate",
        "doco:abstract": "An optimal control problem with distributed control in the\nright-hand side of Poisson equation is considered. Pointwise constraints\non the gradient of state and control are imposed in this problem. The convergence of finite\nelement approximation for this problem is proved. Discrete saddle point problem is constructed\n and preconditioned Uzawa-type iterative algorithm for its solution is investigated.",
        "doco:list_of_authors": "R. Dautov,  A.~Lapin",
        "doco:email": "",
        "doco:subtitle": "",
        "doco:keywords": "optimal control, finite element method, iterative method, constrained saddle point problem",
        "doco:introduction": "\nMany physical processes modeled by partial differential equations\nrequire constraints on their solutions which a play  role of  state in\nthe corresponding optimization problems. Pointwise constraints on\nthe gradient of the state are important, in particular, in  cooling and\nheating processes in order to avoid damage of the products caused by\nlarge material stresses (cf., e.g. \\cite{CC} -- \\cite{CC2} and\nbibliography therein for cooling in continuous casting process).\n\nThe state  constraints  in general deliver  low regularity of\nadjoint variables and this fact brings difficulties in studying the discrete approximations\nof optimal control problems with state  constraints.\nA series of articles is devoted to investigation of  the\napproximation and iterative solution methods   for the optimal\ncontrol problem with pointwise constraints to the state\n(\\cite{bergounioux1} -- \\cite{Hinze1}). Compared to pointwise constraints on the state the  gradient\nconstraints involve the gradient operator, which has a non-trivial\nkernel, and this further complicates the problem. There is a few\narticles dealing with such kind of problems (\\cite{Casas}--\n\\cite{Hintermuller}). Thus, in  \\cite{Casas} a  theoretical analysis\nof an optimal control of semilinear elliptic equation with pointwise\nconstraints on the gradient of the state is made. The investigation\nof the convergence and rate of convergence of finite element\nap\\-proximations to optimal control problems with the constraints on\nthe gradient of the state is the topic of  articles\n\\cite{Deckelnick}  --- \\cite{Ortner}. In \\cite{Deckelnick}\nvariational discretization of the controls is considered combined\nwith the lowest order Raviart--Thomas finite element approximations\nof a mixed formulation of the state equation. Controls are not\ndiscretized explicitely, but implicitly through the optimality\nconditions associated with the discrete approximation to the optimal\ncontrol problem. This in particular leads to piecewise constant\napproximations to the state and the adjoint state. In \\cite{Gunther}\nthe $L_r$-norm of the control is included in  cost functional with\n$r > d $$(d = 2, 3 $ is the dimension of the problem) to guarantee\nthe required regularity of the state. Variational discretization of\nthe control problem then is investigated, as well as piecewise\nconstant approximations of the control. In both cases standard\npiecewise linear and continuous finite elements for the\ndiscretization of the state is used.  Error bounds for control and\nstate are obtained depending on the value of $r$. Similar estimates\nare obtained in \\cite{Ortner}, where $L_r$-norm in  cost functional\nis included as well. In \\cite{Griesse}  semi-smooth Newton methods\nand regularized active set methods are discussed for the solution of\nan elliptic equation  with gradient constraints. An analysis for a\nbarrier method for optimization with constraints on the gradient of\nthe state can be found in \\cite{Schiela}. Adaptive finite element\nmethods for optimization problems for second order linear elliptic\npartial differential equations subject to pointwise constraints of\nthe gradient of the state are considered in \\cite{Hintermuller}. In\na weak duality setting, i.e. without assuming a constraint\nqualification such as the existence of a Slater point, residual\nbased a posteriori error estimators are derived.\n%\\medskip\nIn this paper we consider an elliptic optimal control problem with\ndistributed control, observation in a subdomain and pointwise\nconstraints on the gradient of state.  We approximate this problem\nby finite element scheme  with piecewise constant elements  for\ncontrol function and piecewise linear and continuous finite elements\nfor the discretization of the state function. Pointwise bounds on\nthe gradient of the discrete state are enforced element-wise. We\nprove the strong convergence  of finite element approximation for\nthis problem  by using well-known approach to convergence theory for\nvariational inequalities and minimization problems (cf., e.g.\n\\cite{GLT}).\n\nFurther we construct discrete saddle point problem and its iterative\nsolution method. For these purposes we use the theory of\npreconditioned Uzawa-type iterative methods for  saddle point\nproblems developed in \\cite{La}, \\cite{La-book}, and applied for\nvariational inequalities and optimal control problems in\n\\cite{lap-khas} -- \\cite{lalap2}.\n\nLet us emphasize that the main advantage of the proposed iterative\nmethod is its easy implementation: every iterative step includes\nonly pointwise projections and solutions of the linear algebraic\nequations with the same matrices  for all iterations.\n%%{\\bf \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\u296f\ufffd\ufffd\ufffd $r$.}%%% We analyze a finite element approximation of an elliptic optimal control%problem with pointwise bounds on the gradient of the state variable. We derive convergence%rates if the control space is discretized implicitly by the state equation. In%contrast to prior work we obtain these results directly from classical results for the%$W^1_{\\infty}$-error of the finite element projection, without using adjoint information. If the%control space is discretized directly, we first prove a regularity result for the optimal%control to control the approximation error, based on which we then obtain analogous%convergence rates%\\medskip%\\cite{Ortner}  presents error bounds similar to \\cite{Gunther}, but derived by following%techniques developed in Deckelnick, K., Hinze, M.: Convergence of a finite element approximation to a state constrained elliptic control problem. SIAM J. Numer. Anal. 45, 1937-1953 (2007).%%llllll%%\\cite{Wollner}:%Comput Optim Appl (2010) 47: 133-159 DOI 10.1007/s10589-008-9209-2%A posteriori error estimates for a finite element discretization of interior point methods for an elliptic%optimization problem with state constraints%W. Wollner%%{\\bf \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\u296f\ufffd\ufffd\ufffd $r$.}%%In this paper we are concerned with a posteriori error estimates for the%solution of some state constraint optimization problem subject to an elliptic PDE. The%solution is obtained using an interior point method combined with a finite element%method for the discretization of the problem. We will derive separate estimates for%the error in the cost functional introduced by the interior point parameter and by the%discretization of the problem.%%\\bigskip%{\\bf Iterations}%%\\cite{Griesse}  semi-smooth Newton methods and%regularized active set methods are discussed for the solution of an elliptic equation  with%gradient constraints. An analysis for a barrier method for optimization with constraints%on the gradient of the state can be found in \\cite{Schiela}%%\\bigskip%\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\u1b2e\ufffd\ufffd\ufffd\ufffd WEAK-DUALITY BASED ADAPTIVE FINITE ELEMENT%METHODS FOR PDE-CONSTRAINED OPTIMIZATION WITH%POINTWISE GRADIENT STATE-CONSTRAINTS*%M. Hintermuller, Hinze,  Hoppe%%%where solidification of melts forms a critical process. In order to accelerate the%production it is highly desirable to speed up the cooling processes while avoiding%damage of the products caused by large material stresses. Cooling frequently is described%by systems of partial differential equations involving the temperature as a%system variable, so that large (Von Mises) stresses in the optimization can be kept%small by imposing pointwise bounds on the gradient of the temperature.%\\bigskip%%%%Many physical processes modeled by partial differential equations require bounds on%the gradient of the state variable. For example, large temperature gradients during%cooling or heating of an object may lead to its destruction, or, in solid mechanics,%the deformation gradient determines the change between elastic and plastic material%behavior. Therefore, optimization of such processes may require pointwise constraints%on the gradient of the state.%%{\\it \ufffd\ufffd\ufffd \ufffd\ufffd\u0a20\ufffd\ufffd}%%%Pointwise constraints on the gradient of the state are important, e.g.,%in secondary cooling process (\\cite{}) in order to avoid damage of the products caused by large material stresses.%In model based optimization, cooling processes frequently are described by systems of partial differential equations involving the temperature as a system variable, so that large  stresses in the optimization%process can be avoided by imposing pointwise bounds on the gradient of the temperature.%lllllllll%",
        "doco:conclusion": "",
        "doco:acknowledgements": "",
        "doco:chapter": [
            "\n Let $\\Omega \\subset\\mathbb{R}^2$ be a polygonal domain  and $ \\Omega_1 \\subseteq \\Omega $ be its polygonal\nsubdomain. Define arbitrary functions $y_d, u_d \\in L_2(\\Omega)$,\nand\n\\begin{equation}\\label{contin}\n   \\mbox{functions}\\; y^*, u_1^*,u_2^* \\;\\mbox{from}\\;C(\\overline{\\Omega}),\\\n   \\mbox{such that} \\\\\\; y^*(x)>0, \\, u_1^*(x)<0< u_2^*(x) \\ \\mbox{at}\\   x\\in \\overline{\\Omega}.\n\\end{equation}\nLet state problem is the Dirichlet problem for the Poisson equation:\n\\begin{equation}\\label{ur1}\ny\\in H^{1}_{0}(\\Omega): \\int\\limits_{\\Omega}\\nabla y\\cdot \\nabla z\n\\,dx=  \\int\\limits_{\\Omega}u zdx \\ \\ \\forall\\, z\\in H^{1}_{0}(\\Omega),\n\\end{equation}\nwhere  $ u (x) $ is the control function and solution $ y (x) $ of\n equation \\eqref{ur1} is state of the system. Define the convex\nand closed sets of the  constraints for control and state functions:\n\\begin{gather*}\n  U_{ad}=\\{u\\in L_2(\\Omega): u_1^*(x)\\leqslant u(x) \\leqslant u_2^*(x) \\mbox{ a.e. in  } \\Omega\\},\n\\\\\n  Y_{ad}=\\{y\\in H^{1}_{0}(\\Omega): |\\nabla y(x)|\\leqslant y^*(x) \\mbox{ a.e. in  } \\Omega\\}.\n\\end{gather*}\nLet $\\alpha>0$. Consider the following optimal control problem:\n\\begin{equation}\\label{optimalno}\n\\begin{array}{c}\n\\displaystyle \\min\\limits_{(y,u)\\in K}\\Big\\{J(y,u)=\\dfrac{1}{2}\\int\\limits_{\\Omega_1}(y-y_d)^2\\,dx+\\dfrac{\\alpha}{2}\\int\\limits_{\\Omega}(u-u_d)^2\\,dx\\Big\\},\n \\\\\n K=\\{(y,u):\\; y \\;\\mbox{is a solution of}\\;  \\eqref{ur1} \\;\\mbox{and}\\; y\\in Y_{ad},\\;  \\; u\\in U_{ad}\\}.\n\\end{array}\n\\end{equation}%----------------------------------------------------\\begin{lemma}\n  Problem \\eqref{optimalno} has a unique solution.\n\\end{lemma}%{\\it Proof.} \\begin{proof}\nSet $ K $  is a non-empty convex compact set in\n$H^1_0(\\Omega) \\times L_2(\\Omega)$, containing zero function, and\nthe functional $ J$ is continuous. Therefore the  existence of the\nminimum point of $J$ on the set $K$  follows from  Weierstrass\ntheorem.\n\nTo prove the uniqueness of the solution, we prove that the functional $ J $ is strictly convex on the set $ K $. In fact, let $ (y_1, u_1) \\neq (y_2, u_2) $ be two different elements of $ K $. Then $ u_1 \\neq u_2 $, because otherwise\n$ y_1 = y_2 $ according to the equation \\eqref{ur1}. Now, from the convexity of the functional\nin $ y $ and strict convexity in $ u $ follows inequality $J( (y_1 + y_2)/2, (u_1 + u_2)/2) <1/2 J (y_1, u_1) +1/2 J (y_2, u_2), $ i.e. strict convexity of $ J $ on $ K $.\n\n\nBelow we use the notation $ \\|\\cdot\\|_{0, p} $ for norms of Lebesgue\nspaces $ L_p(\\Omega)$ and $\\|\\cdot\\|_{l, p}$ for norms of Sobolev\nspaces $ W_p^l(\\Omega)$ for $ 1 \\leqslant p \\leqslant \\infty $ and\nintegers $ l> 0 $.\n\\end{proof}%\\medskip\nLet $\\mathcal{T}_h=\\bigcup e_i$ be a conforming and regular\ntriangulation of the domain $\\Omega,$$h$ be the maximum diameter of\nelements $e\\in \\mathcal{T}_h$(\\cite{Ciarlet}). We assume that the triangulation is\ncompatible with $ \\Omega_1 $ in the sense that $\\overline{\\Omega}_1$\nconsists of a number of triangles $ e\\in \\mathcal{T}_ {1h} \\subseteq \\mathcal{T}_h $. We define the finite element spaces\n$\nH_h=\\{y_h\\in H_{0}^{1}(\\Omega): y_{h}(x)\\in P_1 \\mbox{ on } e \\in\n\\mathcal{T}_h\\},$$ U_h=\\{u_h \\in L_2(\\Omega): u_{h}(x)\\in\nP_0\\;\\mbox{on}\\; e \\in \\mathcal{T}_h\\},\n$\n where $ P_k $ is the set of\npolynomials of degree at most $k$ in all variables. We denote by\n$\\pi_h $ the operator of integral averaging of functions from\n$L_1(\\Omega)$, with values in $U_h$:\n$$\n   \\pi_h u(x)=|e_i|^{-1} \\int\\limits_{e_i}u(t)dt\\;\\mbox{ for }\\; x \\in e_i,\\quad |e_i|={\\rm meas}\\; e_i.\n$$\nLet  $y_{dh}=\\pi_h y_d,$$u_{dh}=\\pi_h u_d,$$y_{h}^*= \\pi_h y^*, $$u_{1h}^*=\\pi_h u_{1}^*,$$ u_{2h}^* =\\pi_h u_{2}^*$. Then $\ny_{h}^*(x)>0 $ and $ u_{1h}^*(x)<0< u_{2h}^*(x)$. By the continuity\nin average of functions $ y_d $ and $ u_d $  the following limit\nrelations hold: $ \\| y_{dh}-y_d\\|_{0,2}\\rightarrow 0,\\quad \\|\nu_{dh}-u_d\\|_{0,2}\\rightarrow 0, $ and from uniform continuity of\nfunctions $ y^*, u_1^*$ and $u_2^*$, it follows that\n\\begin{equation}\\label{approx}\n\\| y_{h}^*-y^*\\|_{0,\\infty}\\rightarrow 0,\\quad\n\\| u_{1h}^*-u_1^*\\|_{0,\\infty}\\rightarrow 0,\\quad \\| u_{2h}^*-u_2^*\\|_{0,\\infty}\\rightarrow 0.\n\\end{equation}\nWe define a convex and closed sets of the constraints on the mesh control\nand state functions:\n$\n Y_{ad}^h=\\{y_h\\in U_h: |\\nabla y_h|\\leqslant  y_{h}^* \\mbox{ on } \\Omega\\},\n$$\n U_{ad}^h=\\{u_h\\in U_h: u^*_{1h}\\leqslant u_h \\leqslant u_{2h}^* \\mbox{ on } \\Omega\\}.\n$\nDiscrete state problem  is the approximation by the finite element\nmethod of the boundary value problem (\\ref{ur1}):\n\\begin{equation}\\label{ur3*}\ny_h \\in H_{h}: \\int\\limits_{\\Omega}\\nabla y_h\\cdot \\nabla z_h \\,dx=\n\\int\\limits_{\\Omega}u_h z_h \\,dx \\ \\\n\\forall\\, z_h \\in H_h,\\quad u_h\\in U_h.\n\\end{equation}\nObjective function $J_h:\\, H_h\\times U_h \\rightarrow{\\mathbb R}$\nis defined by the equality\n$$\n  J_h(y_h,u_h)=\\dfrac{1}{2}\\int\\limits_{\\Omega_1}(y_h-y_{dh})^2 \\,dx+\\dfrac{\\alpha}{2}\\int\\limits_{\\Omega}(u_h-u_{dh})^2\\,dx.\n$$\nIt is easy to verify that the discrete optimal control problem\n\\begin{equation}\\label{opti}\n\\begin{array}{c}\n   \\displaystyle \\min\\limits_{(y_h,u_h)\\in  K_h} J_h(y_h,u_h),\\\\\n   \\displaystyle K_h=\\{(y_h,u_h):\\; y_h\\  \\mbox{is a solution of} \\; (\\ref{ur3*}) \\;\\mbox{and}\\; y_h\\in Y_{ad}^h,\\; u_h\\in U_{ad}^h\\}\n\\end{array}\n\\end{equation}\nhas a unique solution $(y_h,u_h)$. The reasoning is the same as that\nfor problem \\eqref{optimalno}, namely,  set $ K_h $ is a nonempty convex\ncompact, and the function $ J_h $ is continuous and strictly convex\non $ K_h $.\n\n",
            "\nLet $ (y_h, u_h) $ be the solution of problem \\eqref{opti} for a fixed $h$ while $ (y, u) $ be the solution of  problem \\eqref{optimalno}.  We prove the strong convergence  $ (y_h, u_h)\\rightarrow (y,u) $ as $h\\rightarrow 0$ by using the traditional approach to the study of\nthe convergence of discrete approximations for variational\ninequalities and minimization problems (see eg., \\cite{GLT},\nChapter 1, \\S 4.3, 4.4). This approach is based on the proving the approximation of $ K $ by the family of sets $\\{K_h\\}_h $ and functional $ J $  by the family of functions $\\{J_h \\}_h $.\n\nThe fact that  sets $ K_h $, defined in\n\\eqref{opti},  approximate the set $ K $, defined in\n\\eqref{optimalno}, delivered in the following two lemmas.\n\\begin{lemma}\\label{l1}\nIf $\\{(y_h, u_h)\\}\\in K_h$ and $(y_h, u_h)\\rightarrow (y,u)$  weakly\nin $H^1_0(\\Omega)\\times L_2(\\Omega)$, then $(y,u)\\in K$.\n\\end{lemma}%{\\it Proof.} \\begin{proof}\nLet $\\{(y_h, u_h)\\}\\in K_h$ and $(y_h, u_h)\\rightarrow\n(y,u)$ weakly in $H^1_0(\\Omega)\\times L_2(\\Omega)$. We first prove\nthat $ (y, u) $ satisfies equation \\eqref{ur1}, and then that\n$(y,u)\\in Y_{ad}\\times U_{ad}$.\n\nFor any $ z \\in H_0^1(\\Omega) $, take the sequence $ \\{z_h \\} \\in\nH_h $, which is strongly  converges to $ z $ in $ H_0^1(\\Omega) $.\nPassing to the limit as $ h \\rightarrow 0 $ in the equation\n\\eqref{ur3*}, we get that $ (y, u) $ satisfies \\eqref{ur1}.\n\nTake an arbitrary $ \\varepsilon> 0 $ and consider the sets\n$Y_{ad}^{\\varepsilon}=\\{y\\in L_2(\\Omega):|\\nabla y(x)|\\leqslant\ny^*(x)+\\varepsilon\\; \\mbox{ a.e. in  } \\Omega\\}$ and\n$U_{ad}^{\\varepsilon}=\\{u\\in L_2(\\Omega):\nu_1^*(x)-\\varepsilon\\leqslant u(x) \\leqslant u_2^*(x)+\\varepsilon\n\\mbox{ a.e. in  } \\Omega\\}$. Due to the limit relations\n\\eqref{approx} it is obvious that $Y_{ad}^h\\subset\nY_{ad}^{\\varepsilon}$ and $U_{ad}^h\\subset U_{ad}^{\\varepsilon}$ for\nsufficiently small $ h \\leqslant h(\\varepsilon)$. Since the convex\nand closed sets $Y_{ad}^{\\varepsilon}$ and $U_{ad}^{\\varepsilon}$\nare weakly closed, so $y\\in Y_{ad}^{\\varepsilon}$ and $u\\in\nU_{ad}^{\\varepsilon}$. It remains to note that $\\displaystyle\nY_{ad}=\\bigcap\\limits_{\\varepsilon>0} Y_{ad}^{\\varepsilon}$ and\n$\\displaystyle U_{ad}=\\bigcap\\limits_{\\varepsilon>0}\nU_{ad}^{\\varepsilon}$ and $y\\in Y_{ad}^{\\varepsilon}$, $u\\in\nU_{ad}^{\\varepsilon}$ for all $\\varepsilon>0$, so $y\\in Y_{ad}$ and\n$u\\in U_{ad}$.\n\\end{proof}\\begin{lemma}\\label{l2}\nFor every $ (y, u) \\in K $ there exists a sequence $ \\{(y_h, u_h) \\}\\in K_h $ such that\n$ (y_h, u_h) \\rightarrow (y, u) $ strongly in $H^1_0(\\Omega)\\times L_2(\\Omega)$.\n\\end{lemma}%{\\it Proof.} \\begin{proof}\nThe proof is divided into two parts. First we prove\nthat any $(y, u) \\in K $ is the limit of the functions $ (y_n, u_n)\n\\in K $, having additional smoothness, and such that $ (y_n, u_n)\n\\in {\\rm int}\\, Y_ {ad} \\times{\\rm int}\\, U_ {ad} $. Then, for such\nfunctions we construct the sequence $ \\{(y_h, u_h) \\} \\in K_h $\nconverging to $(y, u)$ strongly in $ H^1_0 (\\Omega) \\times\nL_2(\\Omega) $.\n\nIn the proof of the first assertion we will use the following two\nfacts (\\cite{Ran-Scott}, \\cite{Nochetto}):\n\\begin{enumerate}\n\\item\nIf $ g \\in L_\\infty (\\Omega) $, then the solution $ y $ of the\nboundary value problem\n\\begin{equation}\\label{puasson}\n   -\\triangle y=g(x),\\ \\ x\\in \\Omega, \\quad y(x)=0,\\ \\ x\\in \\partial\\Omega,\n\\end{equation}\nbelongs to $ W_{p}^2(\\Omega)$ with some $p = 2 + \\varepsilon, \\,\n\\varepsilon> 0 $, i.e. $y\\in W_{\\infty}^1(\\Omega)$, and the\nfollowing estimate holds\n\\begin{equation}\\label{KZ}\n  \\|y\\|_{1,\\infty}\\leqslant c\\,\\|u\\|_{2,p}\\leqslant c\\,\\|g\\|_{0,\\infty}.\n\\end{equation}\n\\item\nIf $y\\in W_{p}^2(\\Omega)$ is the solution of \\eqref{puasson} with\n$g\\in L_\\infty(\\Omega)$, and $y_h$ is the FEM solution\n$$\n   y_h\\in H_h:\\;\\int\\limits_\\Omega \\nabla y_h\\cdot \\nabla v_h\\,\\,dx=\\int\\limits_\\Omega g v_h\\,\\,\\,dx\\quad \\forall\\,\\,v_h\\in H_h,\n$$\nthen\n\\begin{equation}\\label{otze}\n   \\|y-y_h\\|_{1,\\infty}\\leqslant c\\,h^{1-2/p}\\,\\|g\\|_{0,\\infty}\\rightarrow 0\\quad \\text{when}\\; h\\rightarrow 0.\n\\end{equation}\n%\\item\n%?\ufffd\ufffd\ufffd\ufffd\ufffd\u28ae $C^{\\infty}(\\overline{\\Omega})$ \ufffd\ufffd\ufffd\u2b6e \ufffd $L_{\\infty}(\\Omega)$.\n\\end{enumerate}\n\nBy  conditions \\eqref{contin} there exists a sufficiently small $\n\\delta> 0 $ such that $u_1^*(x)+3\\delta\\leqslant 0,$\n   $ u_2^*(x) -3\\delta \\geqslant 0 $ and $y^*(x)-3\\delta \\geqslant 0$ a.e. in  $\\Omega.$\nTake an arbitrary pair of $ (y, u) \\in K $ and let $ (y_{\\rho},\nu_{\\rho}) = (\\rho\\, y, \\rho\\, u) $ with $0<\\rho<1$. Then $(y_{\\rho},\nu_{\\rho})\\in K$ and  $(y_{\\rho}, u_{\\rho})\\rightarrow (y,u)$ in\n$H^1_0(\\Omega)\\times L_2(\\Omega)$ when $\\rho\\rightarrow 1-0$. We fix\na maximum value $ \\rho $ such that $ u_{\\rho}(x)\\in\n[u_1^*(x)+3\\delta,  u_2^*(x) -3\\delta]$ and $|\\nabla\ny_{\\rho}(x)|\\leqslant y^*-3\\delta$ a.e. in $\\Omega.$ We now take a\nsequence $ u_n \\in C^{\\infty}(\\overline{\\Omega}) $, which converges\nto $ u_ {\\rho} $ in $ L_ {\\infty} (\\Omega) $ and let $ y_n $\nsolution of \\eqref{puasson} with right hand side $u_n$. These\nfunctions belong to $ W_{p}^2(\\Omega), $ $ p = 2 + \\varepsilon, \\,\n\\varepsilon> 0 $, and by virtue of \\eqref{KZ} satisfy the\nrelations: $\\|y_n-y_{\\rho}\\|_{1,\\infty}\\leqslant c\\,\n\\|u_n-u\\|_{0,\\infty}\\rightarrow 0$ when $n\\rightarrow \\infty. $\nTherefore, there exists a number $ n = n (\\delta) $ such that holds\nthe following inequalities $|\\nabla y_n(x)|\\leqslant y^*(x)-2\\delta$\nand $ u_n(x)\\in [u_1^*(x)+2\\delta,  u_2^*(x) -2\\delta]$  a.e. in\n$\\Omega$.\n\nSo, to prove the lemma it suffices to construct the sequence $\n\\{(y_h, u_h) \\} \\in K_h $, which is converges to a pair $ (y, u) \\in\nK $ strongly in $ H^1_0 (\\Omega) \\times L_2 (\\Omega) $  such that\n$$\n\\begin{array}{c}\n\\displaystyle u(x)\\in [u_1^*(x)+2\\delta,  u_2^*(x) -2\\delta],\\quad |\\nabla y(x)|\\leqslant y^*(x)-2\\delta\\quad \\mbox{a.e. in }\\;\\Omega,\\\\\n\\displaystyle u\\in C(\\overline{\\Omega}), \\quad y\\in W_{p}^2(\\Omega),\\quad  p=2+\\varepsilon,\\; \\varepsilon>0.\n\\end{array}\n$$\nLet  $u_h=\\pi_h u$.  Then $ u_h(x)\\in [u_{1h}^*(x)+2\\delta,\nu_{2h}^*(x) -2\\delta]$  in $\\Omega$ and $\\|u_h-u\\|_{0,\n\\infty}\\rightarrow 0,\\ h\\rightarrow 0$. Denote by $ \\tilde y_h $\nsolution of \\eqref{puasson} with the right hand side $ u_h $. Then\nthe estimate \\eqref{KZ} implies that $\\|y-\\tilde y_h\\|_{1,\\infty}\n  \\leqslant c\\, \\| u-u_h \\|_{0, \\infty} \\rightarrow 0,\\; h \\rightarrow\n0 $. Thus, with $ h \\leqslant h_1 (\\delta) $ following inequalities\nholds: $|\\nabla (y-\\tilde y_h)(x)|\\leqslant\\delta$ and $|\\nabla\n\\tilde y_h(x)|\\leqslant y^*(x)-\\delta $ in $\\Omega.$ Now let $ y_h $\nis the FEM solution\n$$\n y_h\\in H_h:\\;\\int\\limits_\\Omega \\nabla y_h\\cdot \\nabla v_h\\,\\,dx=\\int\\limits_\\Omega u_h v_h\\,\\,\\,dx\\quad \\forall\\,\\,v_h\\in H_h.\n$$\nIn accordance with \\eqref{otze}\n$\n   \\|\\tilde y- y_h\\|_{1,\\infty}\\leqslant c\\,h^{1-2/p}\\,\\|u_h\\|_{0,\\infty}\\leqslant c\\,h^{1-2/p}\\rightarrow  0$ when $h\\rightarrow\n   0.$\nThis means that  the inequality $ |\\nabla (y_h-\\tilde\ny)(x)|\\leqslant\\delta.$ Hence $|\\nabla  y_h(x)|\\leqslant y^*(x)$ in\n$\\Omega $ is true for  $ h \\leqslant h_2 (\\delta)$. Thus, the pair $\n(y_h, u_h) $ belongs to $ K_h $ for sufficiently small $ h $ and\nstrongly converges to $ (y, u) $ in $ H^1_0(\\Omega) \\times\nL_2(\\Omega) $ when $ h \\rightarrow 0 $.\n\\end{proof}\\begin{theorem}\\label{theorem_31}\nSolutions $ \\{(y_h, u_h) \\} $ of the problem \\eqref{opti} strongly\nconverge to the solution $ (y, u) $ of \\eqref{optimalno} in $\nH_0^1(\\Omega) \\times L_2 (\\Omega) $ when $ h \\rightarrow 0 $.\n\\end{theorem}%{\\it Proof.}\\begin{proof} a) {\\it Weak convergence.} Let  $(y_h, u_h)\\in K_h$  be\nthe solution of \\eqref{opti}. Then $ (y_h, u_h) $ is bounded in $\nH_0 ^ 1 (\\Omega) \\times L_2 (\\Omega) $ uniformly with respect to $ h\n$. This allows to select from the sequence $ \\{(y_h, u_h) \\} $\nweakly converging in $ H_0 ^ 1 (\\Omega) \\times L_2 (\\Omega) $\nsubsequence. Keep for it the notation $ \\{(y_h, u_h) \\} $. By Lemma\n\\ref{l1} its limit $ (y, u)$ belongs to $K $. Further, since $ y_h\n\\rightarrow y $, $ y_ {dh} \\rightarrow y_d $, $ u_ {dh} \\rightarrow\nu_d $ strongly and $ u_h \\rightarrow u $  weakly in $ L_2 (\\Omega)\n$, and the quadratic functional $ j (w) = \\int_ {\\Omega} w ^ 2 \\,dx\n$ is weakly lower semicontinuous, then\n $$\n\\liminf\\limits_{h\\rightarrow 0} J_h(y_{h},u_{h})=\n\\liminf\\limits_{h\\rightarrow 0}\\Big\\{ \\dfrac{1}{2}\\int\\limits_{\\Omega_1}(y_h-y_{dh})^2 \\,dx\n+\\dfrac{\\alpha}{2}\\int\\limits_{\\Omega}(u_h-u_{dh})^2\\,dx\\Big\\}\\geqslant J(y,u).\n $$\n Take an arbitrary   $ (z, w) \\in K $. By Lemma \\ref{l2} there exists\n $ \\{(z_ {h}, w_ {h}) \\} \\in K_ {h}: (z_ {h}, w_ {h}) \\rightarrow (z, w) \\mbox{  strongly in  } H ^ 1_0\n(\\Omega) \\times L_2 (\\Omega) $, so $ \\displaystyle \\lim \\limits_ {h\n\\rightarrow 0} J_h (z_ {h}, w_ {h}) = J (z, w). $ As a result, $\nJ(y,u)\\leqslant \\liminf\\limits_{h\\rightarrow 0}\nJ_h(y_{h},u_{h})\\leqslant \\lim\\limits_{h\\rightarrow 0}\nJ_h(z_{h},w_{h})=J(z,w)$ for all $(z,w)\\in K, $ and $ (y, u) $ is\nthe solution of  problem \\eqref{optimalno}. Since the solution $ (y,\nu) $ is unique, then the whole sequence $\\{(y_h, u_h) \\} $ of\nsolutions to  \\eqref{opti} weakly in $ H ^ 1_0 (\\Omega) \\times L_2\n(\\Omega) $ converges to $ (y, u) $. Indeed, suppose that $ \\{(y_h,\nu_h) \\} $ does not converge weakly to $ (y, u) $. This means that\nthere exists a subsequence $ \\{(y_ {h_n}, u_ {h_n}) \\} $, the number\n$ \\varepsilon_0> 0 $ and an element $ (z, w) \\in H ^ 1_0 (\\Omega)\n\\times L_2 (\\Omega) $ such that\n\\begin{equation}\\label{aux1}\n \\Big|\\int\\limits_{\\Omega} \\nabla (y_{h_n}-y)\\cdot \\nabla z \\,dx +\n \\int\\limits_{\\Omega} (u_{h_n}-u)\\,w \\,dx \\Big|>\\varepsilon_0 \\quad\\forall\\, h.\n\\end{equation}\nBut the subsequence $ \\{(y_ {h_n}, u_ {h_n}) \\} $ is bounded, so, in accordance with\nproven above, it contains  a subsequence which weakly in $ H_0 ^ 1 (\\Omega)\n\\times L_2 (\\Omega) $ converges to $ (y, u) $. This contradicts to (\\ref{aux1}).\n\nb) {\\it Strong convergence $u_h\\rightarrow u$.} First we prove,  that\n$\\lim\\limits_{h\\rightarrow 0} J(y_{h},u_{h}) = J(y,u)$. Let a\nsequence  $(z_{h},w_{h})\\in K_{h}$ be such, that\n$(z_{h},w_{h})\\rightarrow (y,u)$ strongly in $H^1_0(\\Omega)\\times\nL_2(\\Omega)$ (lemma \\ref{l2}). Then\n$\n\\limsup\\limits_{h\\rightarrow 0} J(y_{h},u_{h})\\leqslant\n\\lim\\limits_{h\\rightarrow 0} J(z_{h},w_{h})=J(y,u).\n$\nTogether with the inequality $ \\displaystyle J (y, u) \\leqslant\n\\liminf \\limits_ {h \\rightarrow 0} J (y_ {h}, u_ {h}) $ this gives $\n\\displaystyle \\lim \\limits_ {h \\rightarrow 0} J (y_ {h}, u_ {h}) = J\n(y, u). $ Since $ y_h $ strongly in $ L_2 (\\Omega) $ converge to $ y\n$, then $ \\int_ {\\Omega} (y_h-y_ {dh}) ^ 2 \\,dx \\rightarrow \\int_\n{\\Omega} (y-y_ {d}) ^ 2 \\,dx $. From this and limit relation $\n\\displaystyle J (y_ {h}, u_ {h}) \\rightarrow J (y, u) $ it follows\nthat $ \\displaystyle \\int_ {\\Omega} (u_h-u_ {dh}) ^ 2 \\,dx\n\\rightarrow \\int_ {\\Omega} (u-u_d) ^ 2 \\,dx $. Together with weak in\n$ L_2 (\\Omega) $ convergence of $ u_h $ to $ u $, this implies a\nstrong convergence $ u_h $ to $ u $ in $ L_2 (\\Omega) $.\n\nc) {\\it Strong convergence $y_h\\rightarrow y$ in $H_0^1(\\Omega)$.}\nTake a sequence $ \\{\\tilde y_h \\}: \\; \\tilde y_h \\rightarrow y $\nstrongly in $ H ^ 1_0 (\\Omega) $ and use the state equations \\eqref{ur3*} and \\eqref{ur1} and Friedrichs inequality\n\\begin{equation}\\label{Fr}\n\\int\\limits_{\\Omega} y_h^2 \\,dx\\leqslant c^2_f\\int\\limits_{\\Omega}|\\nabla  y_h|^2 \\,dx\\;\\;\\forall\\, y_h\\in H_h.\n\\end{equation}\nWe obtain:\n\\begin{multline*}\n\\int\\limits_{\\Omega}|\\nabla (y_h-\\tilde y_h)|^2 \\,dx=\n\\int\\limits_{\\Omega} u_h (y_h-\\tilde y_h) \\,dx -\n\\int\\limits_{\\Omega}\\nabla \\tilde y_h\\cdot\\nabla (y_h-\\tilde y_h)\n\\,dx=\\\\ =\\int\\limits_{\\Omega}\\nabla (y-\\tilde y_h)\\cdot\\nabla\n(y_h-\\tilde y_h) \\,dx+ \\int\\limits_{\\Omega}(u_h-u) (y_h-\\tilde\ny_h)\\,dx\\leqslant\\\\ \\leqslant  \\Big(\\int\\limits_{\\Omega}|\\nabla\n(y_h-\\tilde y_h)|^2 \\,dx\\Big)^{1/2}\n \\Big(\\Big(\\int\\limits_{\\Omega}|\\nabla (y-\\tilde y_h)|^2 \\,dx\\Big)^{1/2}\n +c_f\\|u-u_h\\|_{L_2(\\Omega)}\\Big).\n\\end{multline*}\nHence it follows $y_h-\\tilde y_h\\rightarrow 0$ in $H^1_0(\\Omega)$\nand, so, $y_h\\rightarrow y$  in $H^1_0(\\Omega)$.\n\\end{proof}",
            "\nWe introduce an auxiliary function $ \\bar p_h = \\nabla y_h \\in U_h\n\\times U_h $ and define a set of constraints $P^h_{ad}=\\{\\bar p_h\\in\nU_h\\times U_h: |\\bar p_h(x)|\\leqslant y_h^*(x)\\;\\mbox{a.e. in}\\;\n\\Omega \\}. $ Now the problem \\eqref{opti} can be written as\n\\begin{gather}\\label{opti1}\n\\displaystyle \\min\\limits_{(y_h,u_h,\\bar p_h)\\in\nW_h}\\Big\\{J_h(y_h,u_h)=\n\\dfrac{1}{2}\\int\\limits_{\\Omega_1}(y_h-y_{dh})^2\n\\,dx+\\dfrac{\\alpha}{2}\\int\\limits_{\\Omega}(u_h-u_{dh})^2\\,dx\\Big\\},\\\\\n\\nonumber W_h=\\{(y_h,u_h,\\bar p_h):\\, \\bar p_h\\in P_{ad}^h,\\; u_h\\in\nU_{ad}^h,\\; \\bar p_h=\\nabla y_h, \\; y_h\\ \\mbox{\\mbox{is a solution\nof} }\\  (\\ref{ur3*})\\}.\n\\end{gather}\nDefine the corresponding Lagrangian function by the equality\n\\begin{multline}\\label{Lag}\n\\mathcal L_h(y_h, u_h,\\bar p_h,\\lambda_h, \\bar \\mu_h)=J_h(y_h,u_h)\n+\\int\\limits_{\\Omega}\\nabla y_h\\cdot \\nabla \\lambda_h\n\\,dx-\\\\-\\int\\limits_{\\Omega}u_h \\lambda_h \\,dx +\\int\n\\limits_{\\Omega} \\bar \\mu_h (\\nabla y_h-\\bar p_h) \\,dx,\n\\end{multline}\nwhere the Lagrange multipliers $ \\lambda_h \\in H_h,\\; \\bar \\mu_h \\in\nU_h \\times U_h $, and the saddle point are looking under constraints\non direct variables $ \\bar p_h \\in P_ {ad} ^ h, u_h \\in U_ {ad} ^ h\n$.\n\n\nFor further formulation the saddle point problem in algebraic form\nwe assign to the functions of the finite element spaces $H_h $ and $ U_h $ the vectors of their nodal parameters.\nLet $ \\displaystyle\n\\omega_h = \\{t_i \\} _ {i = 1} ^ m $ be the set of vertices of\ntriangles $ e \\in T_h, $ lying in $ \\Omega $, $ m = {\\rm card} \\,\n\\omega_h $, $ \\displaystyle \\xi_h = \\{t_i \\} _ {i = 1} ^ s $ be the set\nof barycenters of the triangles $ e \\in T_h $. Put in\ncorrespondence function $ y_h \\in H_h $ and vector $ y \\in {\\mathbb R} ^\nm $ with coordinates $ y_i = y_h (t_i), \\, t_i \\in \\omega_h $(with\nany node numbering $ t_i $), and the functions $ u_h \\in U_h $ ---\nvector $ u \\in {\\mathbb R} ^ s $ with coordinates $ u_i = u_h (t_i),\n\\, t_i \\in \\xi_h $. We will use the notation $ y \\Leftrightarrow y_h\n$, $ u \\Leftrightarrow u_h $.\n\nFurther through $ y_d, u_d, y^*, u_1^*, u_2^* $ we denote vectors of\nnodal parameters of the corresponding mesh functions.\n\nDefine the matrices $L\\in {\\mathbb R}^{m\\times m}$,  $M_u\\in\n{\\mathbb R}^{s\\times s}$, $M_y\\in {\\mathbb R}^{m\\times m}$, $S\\in\n{\\mathbb R}^{s\\times m}$, $R_i\\in {\\mathbb R}^{m\\times s}\\, (i=1,2)$\nby the equalities:\n\\begin{gather*}\n(Ly,z)=\\int\\limits_{\\Omega}\\nabla y_h\\cdot \\nabla z_h \\,dx,\\quad\n(M_u u,v)=\\int\\limits_{\\Omega} u_h(x) v_h(x) \\,dx,\n\\quad(M_y y,z)= \\int\\limits_{\\Omega_1} y_h z_h \\,dx, \\\\\n (R_i y,v) = \\int\\limits_{\\Omega} \\dfrac{\\partial y_h}{\\partial x_i}(x) v_h(x) \\,dx,\\quad\n (Su,y) = \\int\\limits_{\\Omega} u_h(x) y_h(x) \\,dx,\\quad (S_1u,y) = \\int\\limits_{\\Omega_1} u_h(x) y_h(x) \\,dx.\n\\end{gather*}\nThese equalities must be satisfied for all $ y, z \\in {\\mathbb R} ^\nm $ and $ u, v \\in {\\mathbb R} ^ s. $ Here $ y_h, z_h \\in H_h, \\,\ny_h \\Leftrightarrow u, z_h \\Leftrightarrow z $ and, respectively, $\nu_h, v_h \\in U_h, u_h \\Leftrightarrow u, v_h \\Leftrightarrow v. $ By\nconstruction, $ M_u $ is a diagonal positive definite matrix.\n\nWe use the notations $\\overline M_u={\\rm diag}(M_u, M_u)$,\n$\\displaystyle R=\\begin{pmatrix} R_1\\\\R_{2}\\end{pmatrix}$. From the\ndefinitions of the matrices and the Friedrichs inequality \\eqref{Fr}\nfollows:\n\\begin{equation}\\label{polezno}\n%\\begin{array}{c}\n%\\displaystyle \n(M_{y} y, y )\\leqslant c^2_f  (Ly, y),\\quad (Su,y)\\leqslant\nc_f (M_u u,u)^{1/2}(L y,y)^{1/2},\n%\\\\ \n(R y,\\bar p)\\leqslant\n(Ly,y)^{1/2}(\\overline M_u \\bar p, \\bar p)^{1/2}.\n%\\displaystyle (M_y y,y)\\leqslant c_q^{-2}\\, c_f^2 (L y,y).\n%\\end{array}\n\\end{equation}\nLagrange function \\eqref{Lag} and a sets of constraints in terms of\nvectors of nodal parameters of mesh functions take the form\n\\begin{multline*}\n\\mathcal L(y, u,\\bar p,\\lambda, \\bar \\mu)=\\frac 12\\; (M_y y,y)+(S_1\ny_d, y)+\\frac{\\alpha}2\\;\n(M_u (u-u_d),u-u_d)+\\\\\n+(Ly-Su, \\lambda)+(Ry-\\overline M_u\\bar p, \\bar \\mu),\n\\end{multline*}$$P_{ad}=\\{\\bar p\\in {\\mathbb R}^{s}\\times{\\mathbb R}^{s}:\\, p_{1j}^2+p_{2j}^2 \\leqslant y_j^{*2} \\ \\; \\mbox{for all}\\  j=1,2,\\ldots, s \\},$$$$U_{ad}=\\{u\\in {\\mathbb R}^s:\\, u_i\\in[ u_{1i}^*, u_{2i}^*]\\ \\;  \\mbox{for all}\\ i=1,2,\\ldots,m\\}.$$\nLet $ \\varphi_p (\\bar p) $ and $ \\varphi_u (u) $ be the indicator\nfunctions of the sets $ P_ {ad} $ and $ U_ {ad} $. Then the\ncorresponding saddle point problem is\n\\begin{equation}\\label{new-optimum11}\n\\begin{pmatrix}\n M_y& 0&0&L&R^T  \\\\\n   0&0&0&0&-\\overline M_u\\\\\n    0 & 0&\\alpha M_u& -S^T&0\\\\\n L&0&- S&0&0\\\\\n R&-\\overline M_u&0&0&0\n\\end{pmatrix}\n\\begin{pmatrix}\n y \\\\\n\\bar p\\\\\nu\\\\\n   \\lambda\\\\\n   \\bar \\mu\n\\end{pmatrix}+\n\\begin{pmatrix}\n-S_1 y_d\\\\\n    \\partial\\varphi_p(\\bar p)\\\\\n    \\partial \\varphi_u(u)-M_u u_d\\\\\n  0\\\\0\n\\end{pmatrix} \\ni 0.\n\\end{equation}\nThe submatrix\n$$\n\\begin{pmatrix}\n  M_y & 0 &0 \\\\\n   0&0&0\\\\\n    0 & 0&\\alpha M_u\n\\end{pmatrix}\n$$\nof this problem is only positive semidefinite. In order to convert\n\\eqref{new-optimum11} to an equivalent saddle point problem with a\npositive definite submatrix we use both equation $ Ly = Su $ and $\nRy = \\overline M_u \\bar p $. Obvious transformations of the first\ntwo relations in \\eqref{new-optimum11} lead to the system\n\\begin{equation}\\label{preobr}\n\\begin{pmatrix}\n M_y+rL& 0&-rS&L&R^T  \\\\\n   -r R&r \\overline M_u&0&0&-\\overline M_u\\\\\n    0 & 0&\\alpha M_u& -S^T&0\\\\\n L&0&- S&0&0\\\\\n R&-\\overline M_u&0&0&0\n\\end{pmatrix}\n\\begin{pmatrix}\n y \\\\\n\\bar p\\\\\nu\\\\\n   \\lambda\\\\\n   \\bar \\mu\n\\end{pmatrix}+\n\\begin{pmatrix}\n-S_1 y_d\\\\\n    \\partial\\varphi_p(\\bar p)\\\\\n    \\partial \\varphi_u(u)-M_u u_d\\\\\n  0\\\\0\n\\end{pmatrix} \\ni 0.\n\\end{equation}\nFurther we consider  following scalar matrices:\n$$\n\\displaystyle\nK_r=\\begin{pmatrix}\nr & -0.5 r&-0.5  r c_f  \\\\\n  -0.5 r&r&0&\\\\\n    - 0.5  r c_f & 0& \\alpha\n\\end{pmatrix},\\;\\;\n\\overline{K}_r=\\displaystyle\\begin{pmatrix}\nr+ c^2_f & 0.5 r&0.5 r c_f  \\\\\n  0.5 r&r&0&\\\\\n    0.5  r c_f & 0& \\alpha\n\\end{pmatrix}.\n$$\\begin{lemma}\\label{gradient-constr}\nLet $0<r< 3 {\\alpha}/{c_f^2}.$ Then the matrices\n$$  A=\n\\begin{pmatrix}\n  M_y+rL& 0&-rS  \\\\\n   -r R&r \\overline M_u&0\\\\\n    0 & 0&\\alpha M_u\n\\end{pmatrix} \\quad \\text{and} \\quad\n  A_0=\\begin{pmatrix}\n    L & 0&0  \\\\\n    0&\\overline M_u&0&\\\\\n    0 & 0& M_u\n\\end{pmatrix}\n$$\nare spectrally equivalent, i.e.\n$\n  m(r)(A_0x,x)\\leqslant (Ax,x)\\leqslant M(r)(A_0x,x)\\quad \\forall\\;  x=(y,\\bar p, u)^T.\n$\nHere $ m (r)> 0 $  is the minimum eigenvalue of $ K_r, $ and $ M (r)\n$ is the maximum eigenvalue of $ \\overline{K}_r. $\n\\end{lemma}%{\\it Proof.} \\begin{proof} First, we note that due to Sylvester criterion\ncondition $ 0 <r <3 {\\alpha}/{c_f ^ 2} $ provides positive\ndefiniteness of the matrix $ K_r $. Next, using the estimates \\eqref{polezno}, for any vector $ x = (y, \\bar p, u) ^ T $ we obtain:\n\\begin{multline*}\n (A x, x)=r(Ly,y)+(M_y y,y)+r(\\overline M_u \\bar p, \\bar p)+\\alpha (M_u u,u)-r(Su,y)-r(Ry, \\bar p)\\geqslant\n\\\\\n\\geqslant r(Ly,y)+r(\\overline M_u \\bar p, \\bar p)+\\alpha (M_u\nu,u)-r\\, c_f(M_u u,u)^{1/2} (L y,y)^{1/2}- \\\\\n-r(Ly,y)^{1/2}(\\overline M_u \\bar p, \\bar p)^{1/2}\\geqslant\nm(r)\\big((Lu,u)+(\\overline M_u \\bar p, \\bar p)+(M_u u,u)\n\\big)=m(r)(A_0x,x).\n\\end{multline*}\nSimilarly we can prove the second inequality.\n\nIntroduce the notations:\n$$\nA=\\begin{pmatrix}\n M_y+rL& 0&-rS  \\\\\n   -r R&r \\overline M_u&0\\\\\n    0 & 0&\\alpha M_u\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}  L& 0&- S\\\\\nR&-\\overline M_u&0\n\\end{pmatrix},\n$$\n$$\nx=(y,\\bar p,u)^T,\\quad \\eta=(\\lambda,\\quad \\bar \\mu)^T,\\quad   f=(M_y y_d,0, M_u u_d)^T,\\quad\n \\varphi(x)=\\varphi_u(u)+\\varphi_p(\\bar p).\n$$\nThen the problem \\eqref{preobr} can be written as\n\\begin{equation}\\label{vsp}\n\\begin{pmatrix}\n  A & -B^T  \\\\\n  -B & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n x \\\\\n   \\eta\n\\end{pmatrix}+\n\\begin{pmatrix}\n \\partial \\varphi(x)\\\\\n  0\n\\end{pmatrix} \\ni\n\\begin{pmatrix}\n  f\\\\\n0\n\\end{pmatrix}.\n\\end{equation}\nWe assume that the parameter $ r $ is chosen so that $ 0 <r <3\n{\\alpha} / {c_f ^ 2} $. Then the matrix $ A $ is positive definite.\nIn its turn, the matrix $ B $ has full column rank, since its block\n$ \\begin{pmatrix} L & 0 \\\\R & - \\overline M_u \\end{pmatrix} $ is a\nnonsingular matrix. Vector with coordinates $ u = 0, \\bar p = 0, y =\n0 $ belongs to interior of the  constraint sets, as well as to the\nkernel of matrix $ B $. Thus, all the assumptions of Lemma 1 from\n\\cite{La} are fulfilled, and this implies the existence of a\nsolution $ (y, \\bar p, u, \\lambda, \\bar \\mu) $ to problem\n\\eqref{preobr} with  unique $ (y, \\bar p, u) $  (the components $\n\\eta = (\\lambda, \\mu) $ of the solution are not uniquely defined).\nCorresponding to the vector $ (y, \\bar p, u) $ mesh function $ (y_h,\n\\bar p_h, u_h) $ coincides with the solution of the discrete optimal\ncontrol problem \\eqref{opti1}.\n\\end{proof}",
            "\nFrom the system \\eqref{vsp} we obtain the equation $ B (A+\\partial\n\\varphi)^{-1}(B^T \\eta+f) = 0 $  for $\\eta=(\\lambda, \\bar \\mu)^T.$\nTo solve it we apply  one-step iterative method\n\\begin{equation}\\label{Udzawa}\n\\dfrac 1 \\tau\\; D(\\eta^{k+1} - \\eta^k) +\nB (A+\\partial \\varphi)^{-1}(B^T \\eta^k+f) = 0\n\\end{equation}\nwith a symmetric and positive definite matrix $ D $. This is\npreconditioned Uzawa method for solving \\eqref{vsp}. By Theorem 1\nfrom \\cite{La}, it converges from any initial approximation $\n\\eta^0 $, if the following condition holds for the pair\npreconditioner $ D $ -- iteration parameter $ \\tau $:\n\\begin{equation}\\label{B-A}\n(D\\eta, \\eta)>\\frac{\\tau}2\\; (BA_s^{-1}B^T \\eta, \\eta)\\quad\\forall\\; \\eta\\neq 0,\n\\end{equation}\nwhere $ A_s = 0.5 (A + A ^ T) $ is symmetric part of the matrix $ A\n$. Moreover the sequence $ \\{\\eta ^ k \\} _k $ converges to some\nvector $ \\eta ^ * $ from the set of solutions in the energy norm of\nthe matrix $ D $: $\\|\\eta^k-\\eta^*\\|_D\\rightarrow 0$ when\n$k\\rightarrow \\infty. $ Since, in general there are no estimates of\nthe rate of convergence of the method \\eqref{Udzawa}, then it makes\nsense to choose a preconditioner assuming that the problem is solved\nwithout constraints, i.e. $ \\partial \\varphi = 0 $. In this case,\nthe optimal preconditioner matrix is spectrally equivalent to matrix\n$ (B A^ {-1} B ^ T) _s = BA_s ^ {-1} B ^ T $.\n\nBy Lemma \\ref{gradient-constr} matrix $ A_s $ is spectrally equivalent\nto the matrix\n$$\nA_0=\\begin{pmatrix}\n L & 0&0  \\\\\n   0&\\overline M_u&0&\\\\\n    0 & 0& M_u\n\\end{pmatrix}.\n$$\nDirect calculations yield\n$$\nBA_0^{-1}B^T=\\begin{pmatrix}\n L+SM_u^{-1}S^T&R^T\\\\\n R& RL^{-1}R^T+\\overline M_u\n \\end{pmatrix}.\n$$\nMatrix $ BA_0 ^ {-1} B ^ T $ is spectrally equivalent to $ BA_s^{-1}B^ T $, therefore it can be taken as a preconditioner in the Uzawa method. However, in this case, on each step of the iterative method\nit is necessary to solve the coupled system of equations for $\\lambda $ and $ \\bar \\mu $. A more efficient method is to implement a block-diagonal preconditioner, which  can be taken thanks to the\nfollowing \n\\begin{lemma}\\label{ogr-grad}\nLet $D=\\begin{pmatrix}\n L&0\\\\\n 0&\\overline M_u\n \\end{pmatrix}.\n$ Then\n$$\n \\frac{3-\\sqrt 5}{2}\\,(D\\eta, \\eta)\\leqslant (BA_0^{-1}B^T\\eta, \\eta)\\leqslant \\max\\{2+c_f^2, 3\\}\\,(D\\eta, \\eta)\\quad\\forall\\;  \\eta=(\\lambda, \\bar \\mu)^T.\n$$\n\\end{lemma}%{\\it Proof.} \\begin{proof}\nStraightforward calculations lead to the equality\n$$\n(BA_0^{-1}B^T\\eta, \\eta)= (L\\lambda, \\lambda)+(M_u^{-1} S^T \\lambda, S^T \\lambda)+(\\overline M_u \\bar \\mu, \\bar \\mu)+(RL^{-1}R^T \\bar \\mu, \\bar \\mu)+2(R\\lambda, \\bar \\mu).\n$$\nTo estimate the right-hand side we use the following inequalities\n(hereinafter $y\\Leftrightarrow y_h,\\, u\\Leftrightarrow u_h, \\bar\np\\Leftrightarrow \\bar p_h$):\n$$\n\\begin{array}{c}\n\\displaystyle (L^{-1}R^T\\bar p, R^T\\bar p)^{1/2}=\\sup\\limits_{y_h\\in H_h}\\frac{\\int\\limits_{\\Omega} \\nabla y_h \\cdot \\bar p_h \\,dx}{\\Big(\\int\\limits_{\\Omega} |\\nabla y_h|^2 \\,dx\\Big)^{1/2}}\\leqslant \\Big(\\int\\limits_{\\Omega} |\\bar p_h|^2 \\,dx\\Big)^{1/2}=(\\overline M_u \\bar p, \\bar p)^{1/2},\\\\\n\\displaystyle (M_u^{-1}S^Ty, S^Ty)^{1/2}=\\sup\\limits_{u_h\\in U_h}\\frac{\\int\\limits_{\\Omega} y_h u_h \\,dx}{\\Big(\\int\\limits_{\\Omega} u^2_h \\,dx\\Big)^{1/2}}\\leqslant \\Big(\\int\\limits_{\\Omega} y_h^2 \\,dx\\Big)^{1/2}\\leqslant c_f (Ly,y)^{1/2}.\n\\end{array}\n$$\nUsing the first auxiliary  inequality, we obtain the lower bound\n\\begin{multline*}\n(BA_0^{-1}B^T\\eta, \\eta)\\geqslant (L\\lambda, \\lambda)+\n(\\overline M_u \\bar \\mu, \\bar \\mu)+(L^{-1}R^T \\bar \\mu, R^T\\bar \\mu)-2(L^{-1}R^T \\bar \\mu, R^T\\bar \\mu)^{1/2}(L\\lambda, \\lambda)^{1/2}\\geqslant\n\\\\\n\\geqslant (1-\\varepsilon)(L\\lambda, \\lambda)+(1-\\varepsilon)(\\overline M_u \\bar \\mu, \\bar \\mu)+(1+\\varepsilon-1/{\\varepsilon})(L^{-1}R^T \\bar \\mu, R^T\\bar \\mu).\n\\end{multline*}\n\nLet  now  $ \\varepsilon $ be the positive solution of  the equation $ \\displaystyle 1\n+ \\varepsilon- 1/ {\\varepsilon} = 0, $ then\n$$\n(BA_0^{-1}B^T\\eta, \\eta)\\geqslant \\displaystyle \\frac{3-\\sqrt 5}{2}\\;\\big((L\\lambda, \\lambda)+(\\overline M_u \\bar \\mu, \\bar \\mu)\\big).\n$$\nTo obtain upper bound we use both auxiliary inequalities:\n\\begin{multline*}\n(BA_0^{-1}B^T\\eta, \\eta)\n\\leqslant (1+c^2_f)(L\\lambda, \\lambda)+2(\\overline M_u \\bar \\mu, \\bar \\mu)+2 (L\\lambda,\\lambda)^{1/2}(\\overline M_u \\bar \\mu, \\bar \\mu)^{1/2}\\leqslant\n\\\\\n\\leqslant (2+c^2_f)(L\\lambda, \\lambda)+3(\\overline M_u \\bar \\mu, \\bar \\mu).\n\\end{multline*}\nThe results of  Lemmas \\ref{gradient-constr} and \\ref{ogr-grad} ensure the\nspectral equivalence of matrices $BA_s^{-1}B^T$ \ufffd $D$:\n$\n   c_{\\min}D\\leqslant BA_s^{-1}B^T\\leqslant c_{\\max} D,\n$\nwhere $c_{\\min}=({3-\\sqrt 5})/({2}M(r))$ and\n$c_{\\max}=\\max\\{2+c_f^2, 3\\}/m(r)$, and the constants $ m (r) $ and\n$ M (r) $ are defined in Lemma \\ref{gradient-constr}.\n\\end{proof}\\begin{theorem} Let $0<r< 3 {\\alpha}/{c_f^2}$\nand  $m(r)>0$ be minimal eigenvalue of $ K_r $, defined in Lemma\n\\ref{gradient-constr}. Then Uzawa method \\eqref{Udzawa} for problem\n\\eqref{preobr}  converges if\n\\begin{equation}\\label{ty}\n0<\\tau< 2 m(r)/\\max\\{2+c_f^2, 3\\}.\n\\end{equation}\n\\end{theorem}%{\\it Proof.} \\begin{proof}\nAs noted above, it suffices to prove the inequality\n\\eqref{B-A}. But from Lemma \\ref{ogr-grad} it follows that\n$\n  BA_s^{-1}B^T\\leqslant m(r)^{-1}BA_0^{-1}B^T\\leqslant \\max\\{2+c_f^2, 3\\} m(r)^{-1}D,\n$\nso   $ D>{\\tau}/2\\; BA_s^{-1}B^T$ due to \\eqref{ty}.\n\\end{proof}\\subsection{Implementation of the Preconditioned Uzawa method}\nIt is easy to see that one iteration of  method \\eqref{Udzawa}\nreduces to implementation of the following calculations for the\nknown $\\lambda^k$ \ufffd $\\bar \\mu^k$:\n\\begin{enumerate}\n\\item\n$u^{k+1}=(\\alpha M_u+\\partial \\varphi_u)^{-1}(S^T\\lambda^k+M_u\nu_d)=Pr_{U_{ad}}(\\alpha^{-1} M_u^{-1} (S^T \\lambda^k+M_u u_d)$;\n\\item\n$y^{k+1}=(M_y+rL)^{-1}(S_1 y_d+r S u^{k+1}-L\\lambda^k-R^T\\bar\n\\mu^k);$\n\\item\n$\\bar p^{k+1}=(r \\overline M_u+\\partial\n\\varphi_p)^{-1}(\\overline M_u\\bar \\mu^k+rR\ny^{k+1})=Pr_{P_{ad}}(r^{-1} \\bar \\mu^k+ \\overline M_u^{\\,-1} R\ny^{k+1})$;\n\\item\n$\\displaystyle  \\lambda^{k+1} = \\lambda^k+\\tau (y^{k+1}-L^{-1} S\nu^{k+1});$\n\\item\n$\\displaystyle \\bar \\mu^{k+1} =\\bar \\mu^k+\\tau (\\overline{M}_u^{-1}Ry^{k+1}-\\bar p^{k+1}).$\n\\end{enumerate}\nBy virtue of diagonality of the matrices $ M_u $ and $ \\overline M_u =\n{\\rm diag} \\, (M_u, M_u) $  and pointwise constraints for $u\\in U_ {ad} $ and $ \\bar p\\in P_ {ad} $ the determination of $ u ^{k +1} $ and $ \\bar p ^ {k +1} $ reduces to the pointwise\nprojections of known vectors to the corresponding sets of constraints.\nMore precisely, for a fixed $ i $:\n$\n  u^{k+1}_i=Pr_{[-u_{1i}^*,u_{2i}^*]}\\Big({(\\alpha\nm_{ii})^{-1}} \\, (S^T\\lambda^k+M_u u_d)_i\\Big), $ where $m_{ii}$ is\na diagonal element $M_u$, and\n$|\\bar p^{k+1}_i|=Pr_{[0, y_i^*]} |\\bar F|,\\quad\np^{k+1}_{i1}=|\\bar p^{k+1}_i|^{-1} F_1, \\quad p^{k+1}_{i2}=|\\bar p^{k+1}_i|^{-1} F_2,$\nwhere $\\bar F=(F_1, F_2)=(r^{-1} \\bar \\mu^k+ \\overline M_u^{-1} R y^{k+1})_i$.\n\n\n\\subsection{Control of accuracy and stopping criterion}\nWhen the saddle point problem \\eqref{vsp} is solved by any\niterative method, we find not only an approximation of $ (x ^ k,\n\\eta ^ k) $ to the exact solution $ (x, \\eta) $, but also the vector\n$ \\gamma^ k \\in \\partial \\varphi (x ^ k) $ --- the unique selection from the set $ \\partial \\varphi (x ^ k)$. We\ndefine the components of the residual vector by  the equalities\n$\n   r_x^k=f-Ax^k-\\gamma^k+B^T \\eta^k,$$ r_{\\eta}^k=-Bx^k.\n$\nThen the error vector $ (x-x^k, \\eta-\\eta^k)^T $ satisfies  the\nsystem\n$$\n\\begin{pmatrix}\n\\displaystyle   A & \\displaystyle  -B^T  \\\\\n\\displaystyle   B & \\displaystyle 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\displaystyle  x-x^k \\\\\n\\displaystyle    \\eta-\\eta^k\n\\end{pmatrix}+\n\\begin{pmatrix}\n\\displaystyle  \\partial \\varphi(x)-\\gamma^k\\\\\n0\n\\end{pmatrix} \\ni\n\\begin{pmatrix}\n \\displaystyle r_x^k\\\\\n \\displaystyle r_{\\eta}^k\n\\end{pmatrix}.\n$$\nMultiplying this system scalarly by the vector $ (x-x^k, \\eta-\\eta ^\nk) ^ T $ and applying the inequality $ (\\partial \\varphi (x) -\n\\partial \\varphi (x ^ k), x-x ^ k) \\geqslant 0 $, we get\n$\n   (A(x-x^k), x-x^k)\\leqslant (r_x^k, x-x^k )+(r_{\\eta}^k, \\eta-\\eta^k).\n$\nHence\n\\begin{equation}\\label{skorostn}\n \\|x-x^k\\|_{A_s}^2\\leqslant \\|r_x^k\\|_{A_s^{-1}}\\|x-x^k\\|_{A_s}+|(r_{\\eta}^k, \\eta-\\eta^k)|.\n\\end{equation}\nSince the inclusion $A x -B^T\\eta+\\partial \\varphi(x)\\ni f$ is\nsolved exactly at each iteration of Uzawa method \\eqref{Udzawa},\ntherefore $ r^k_x = 0 $ and  estimate \\eqref{skorostn} takes the\nform\n\\begin{equation}\\label{skorost11}\n \\|x-x^k\\|_{A_s}\\leqslant |(r_{\\eta}^k, \\eta-\\eta^k)|\\leqslant  \\|\\eta-\\eta^k\\|_{D}^{1/2} \\|r_{\\eta}^k\\|_{D^{-1}}^{1/2}\n\\end{equation}\n where $D$ is the preconditioner of this method. Since $ \\| \\eta-\\eta ^ k \\| _D \\rightarrow 0 $ for $ k \\rightarrow\n\\infty $, inequality \\eqref{skorost11} gives the information about\nerror $ \\| x-x ^ k \\| _ {A_s} $ through the estimate of the norm of\nthe residual component $\\|r_{\\eta}^k\\|_{D^{-1}}$, namely, $\n\\|x-x^k\\|_{A_s}=o(\\|r_{\\eta}^k\\|_{D^{-1}}^{1/2})$ when $k\n\\rightarrow \\infty. $ In the problem \\eqref{preobr} vector $ r_\n{\\eta} ^ k = (L y ^ {k} - S u ^ {k}, Ry ^ {k} - \\overline M_u \\bar p\n^ {k}) $, so the upper bound for number of iterations is the value\n$$\n\\delta^k=\\|r_{\\eta}^k\\|_{D^{-1}}^{1/2}=\\big((L y^{k}- S u^{k}, y^{k}- L^{-1}S u^{k})+(Ry^{k}\n -\\overline M_u \\bar p^{k}, \\overline M_u^{-1}Ry^{k}-\\bar p^{k})\\big)^{1/2}.\n$$\nNote that the vectors\n$$\nLy^{k}-S u^{k},\\quad y^{k}-L^{-1} S u^{k}=({\\lambda^{k}-\\lambda^{k-1}})/\\tau,\\quad\n Ry^{k}-\\overline M_u \\bar p^{k},\\quad M_u^{-1}Ry^{k}-\\bar p^{k}=({\\bar \\mu^{k+1}-\\bar \\mu^k})/\\tau,\n$$\nare computed when  implementing the algorithm, thus,  control of the value  $ \\delta ^ k $ does not lead to\nadditional computational cost.\n\n\\begin{note}\nDiscrete objective function can be constructed by using\napproximations of the integrals  by composite quadrature formulas, for example, on the basis of  one-point quadrature formulae with the node\ncoinciding with the barycenter $ a_e $ of the triangle $ e\\in {\\cal\nT}_h $:\n$$\n\\int\\limits_{e} g(x) \\,dx\\approx S_{e}(g)= {\\rm meas} e\\,  g(a_e),\\;\n\\int\\limits_{\\Omega_1} g(x) \\,dx\\approx S_{\\Omega_1}(g)=  \\sum\\limits_{e\\in \\bar \\Omega_1\\cap \\mathcal{T}_h}S_e(g).\n$$\nIn this case, the objective function $ J_h: \\, H_h \\times U_h\n\\rightarrow{\\mathbb R} $ defined by\n$$\nJ_h(y_h,u_h)=\\dfrac{1}{2}\\;S_{\\Omega_1}((y_h-y_{dh})^2)+\\dfrac{\\alpha}{2}\\;\\int\\limits_{\\Omega}(u_h-u_{dh})^2\\,dx.\n$$\nDiscrete  optimal control problem $ \\min\\limits_{(y_h,u_h)\\in  K_h}\nJ_h(y_h,u_h),$ where $$ \\displaystyle K_h=\\{(y_h,u_h):\\; y_h\\\n\\mbox{is a solution of} \\; (\\ref{ur3*}) \\;\\mbox{and}\\; y_h\\in\nY_{ad}^h,\\; u_h\\in U_{ad}^h\\},$$ has a unique solution $ (y_h, u_h)\n$. Results about the convergence of discrete scheme and Uzawa\niterative method remain in force. Moreover bounds on parameters $ r\n$ and $ \\tau $ are the same as in preceding case.\n\\end{note}"
        ]
    },
    "errors": [
        "email",
        "shorttit"
    ]
}